residualVariation=function(t){rnorm(1,0,2)},
decimal=2,percentOfMissing=0)
dat2 <- dat2["traj"]
dat <- cbind(dat,dat2)
View(dat)
names(dat)
colnames(dat) <- c(paste("V",0:5,sep=""),paste("X",0:5,sep=""))
sqrt(4000)
library(ltmle)
install.packages("ltmle")
library(ltmle)
help("ltmle")
examples(ltmle())
library(ggplot2)
help("coord_fixed")
log(2000)
log(50.45)
A <- rbinom(100,0.2)
A <- rbinom(100,1,0.2)
B <- rbinom(100,1,0.5)
table(A,B)
tt <- table(A,B)
chisq.test(tt)
install.packages("NPcausal")
install.packages("npcausal")
help(pam)
library(cluster)
help(plot.pam)
help("pam")
library(varimpact)
remotes::install_github("ck37/varimpact")
library(varimpact)
help(varimpact)
knitr::opts_chunk$set(echo = TRUE)
dat <- read.csv("data_clean_v2_2024-01-15.csv")
names(dat)
SES.vars <- c("mixer", "blender", "tv", "cable")
dat.clust <- dat[,SES.vars]
summary(dat$mixer)
summary(dat$blender)
SES.vars <- c("mixer", "urban", "tv", "cable", "cellphone")
dat.clust <- dat[,SES.vars]
View(dat.clust)
dat.clust[dat.clust==99] <- NA
View(dat.clust)
rownames(dat.clust[1:10])
rownames(dat.clust)[1:10]
rownames(dat)[1:10]
dat.clust.notNA <- na.omit(dat.clust)
dim(data.clust.notNA)
dim(dat.clust.notNA)
dim(dat.clust)
View(dat.clust)
View(dat.clust.notNA)
summary(dat.clust.notNA)
??e1071
??hamming.distance
distmat <- hamming.distance(dat.clust.notNA)
library(e1071)
distmat <- hamming.distance(dat.clust.notNA)
library(parallelDist)
nCores <- as.numeric(parallel::detectCores())
parD<-parDist(dat.clust.notNA, method =  "hamming", threads = nCores)
dat.mat <- as.matrix(dat.clust.notNA)
parD<-parDist(dat.mat, method =  "hamming", threads = nCores)
parD
parD[1:10]
library(cluster)
help("pam")
pam.out <- pam(parD,k=4,pamonce =6)
pam.out$medoids
names(pam.out)
pam.out$clustering
table(pam.out$clustering)
length(clusters)
clusters <- pam.out$clustering
length(clusters)
dim(dat.mat)
dat.clust.notNA <- data.frame(dat.clust.notNA,clusters)
View(dat.clust.notNA)
names(dat.clust.notNA)
help("unique")
paste.fun <- function{x}{
paste.fun <- function(x){
paste(x,sep=",")}
new.string <- apply(dat.clust.notNA,1,paste.fun)
new.string[1:10]
paste.fun <- function(x){
paste(as.character(x),collapse=",")}
new.string <- apply(dat.clust.notNA,1,paste.fun)
new.string[1:10]
paste.fun <- function(x){
paste(as.character(x),collapse=", ")}
new.string <- apply(dat.clust.notNA,1,paste.fun)
new.string[1:10]
dat.clust.notNA <- na.omit(dat.clust)
Assets <- apply(dat.clust.notNA,1,sum)
View(dat.clust.notNA)
Assets <- apply(dat.clust.notNA,1,sum)
dat.clust.notNA <- data.frame(dat.clust.notNA,clusters,Assets)
View(dat.clust.notNA)
dat.clust.notNA <- dat.clust.notNA %>% mutate(count = tally)
library(dplyr)
dat.clust.notNA <- dat.clust.notNA %>% mutate(count = tally)
dat.clust.notNA <- na.omit(dat.clust)
Assets <- apply(dat.clust.notNA,1,sum)
Group <- apply(dat.clust.notNA,1,paste.fun)
dat.clust.notNA <- data.frame(dat.clust.notNA,clusters,Assets,Group)
View(dat.clust.notNA)
dat.clust.notNA <- dat.clust.notNA %>% mutate(tot = tally(Group))
dat.clust.notNA <- dat.clust.notNA %>% add_count(Group)
View(dat.clust.notNA)
View(dat.clust.notNA)
dat.merge <- dat.clust.notNA[,c("clusters","Assets","Group","n")]
View(dat.merge)
View(dat.clust.notNA)
dat.clust.notNA <- na.omit(dat.clust)
dim(dat.clust.notNA)
summary(dat.clust.notNA)
nCores <- as.numeric(parallel::detectCores())
dat.mat <- as.matrix(dat.clust.notNA)
parD<-parDist(dat.mat, method =  "hamming", threads = nCores)
pam.out <- pam(parD,k=4,pamonce =6)
View(dat.clust.notNA)
clusters <- pam.out$clustering
length(clusters)
dim(dat.clust.notNA)
Assets <- apply(dat.clust.notNA,1,sum)
Group <- apply(dat.clust.notNA,1,paste.fun)
View(dat.clust.notNA)
dat.clust.notNA <- data.frame(dat.clust.notNA,clusters,Assets,Group)
View(dat.clust.notNA)
View(dat.clust.notNA)
dat.clust.notNA <- dat.clust.notNA %>% add_count(Group)
View(dat.clust.notNA)
dat.clust.notNA <- na.omit(dat.clust)
Assets <- apply(dat.clust.notNA,1,sum)
Group <- apply(dat.clust.notNA,1,paste.fun)
dat.clust.notNA <- data.frame(dat.clust.notNA,clusters,Assets,Group)
rn <- rownames(dat.clust.notNA)
rn[1:20]
dat.clust.notNA <- dat.clust.notNA %>% add_count(Group)
rownames(dat.clust.notNA)[1:20]
rownames(dat.clust.notNA) <- rn
rownames(dat.clust.notNA)[1:20]
dat.merge <- dat.clust.notNA[,c("clusters","Assets","Group","n")]
View(dat.merge)
patterns <- unique(dat.merge)
dim(patterns)
View(patterns)
oo <- order(patterns$clusters,patterns$Assets)
patterns <- patterns[oo,]
patterns
help("merge")
dat.out <- merge(dat,dat.merge,by="row.names",all.x=T)
View(dat.out)
install.packages("devtools")
devtools::install_github("tlverse/tlverse")
library(data.table)
library(dplyr)
library(glmnet)
library(R6)
library(sl3)
remotes::install_github("tlverse/sl3@v1.3.7")
generate_data_simple <- function(N){
# A simple data generating process
X1 <- runif(N,-1,1)
X2 <- runif(N,-1,1)
pi0 <- plogis(0.1*X1*X2-0.4*X1)
A <- rbinom(N,1,prob=pi0)
muY0 <- X1*X2 + 2*X2^2 -X1
CATE <- X1^2*(X1+7/5) + (5*X2/3)^2
muY = muY0+A*CATE
Y <- rnorm(N,sd=1,mean= muY)
return(tibble(X1=X1,X2=X2,A=A,Y=Y))
}
df <- generate_data_simple(500)
View(df)
node_list <- list(
W = setdiff(names(df), c("A", "Y")),
A = "A",
Y = "Y"
)
library(hal9001)
??Rcpp
install.packages("Rcpp")
install.packages("Rcpp")
library(hal9001)
lrnr_uhal <- Lrnr_uhal9001$new()
install_github(“tlverse/sl3@devel”)
install_github("tlverse/sl3@devel")
library(devtools)
install_github("tlverse/sl3@devel")
library(sl3)
setwd("~/UC Berkeley Biostat Dropbox/Alan Hubbard/hubbardlap/PH240A/2024/Labs/Lab7")
setwd("~/UC Berkeley Biostat Dropbox/Alan Hubbard/hubbardlap/grantproposals/ki Gates/Synthetic data supp/GitSynth/pasc-sim/R")
load("April7.rdata")
reg.nmes <- c("int","A","X1","X2")
View(simp.res)
reg.nmes <- c("int","A","X1","X2")
nmes <- c("ave.dat.A","ave.dat.Y","ave.hal.A",
"ave.hal.Y","ave.halcv.A",
"ave.halcv.Y","ave.sl.A","ave.sl.Y",
paste0("regest.dat.",reg.nmes),
paste0("regSE.dat.",reg.nmes),
paste0("regest.hal.",reg.nmes),
paste0("regSE.hal.",reg.nmes),
paste0("regest.halcv.",reg.nmes),
paste0("regSE.halcv.",reg.nmes),
paste0("regest.sl.",reg.nmes),
paste0("regSE.sl.",reg.nmes))
colnames(simp.res) <- nmes
View(simp.res)
dim(simp.res)
remotes::install_github("nhejazi/haldensify")
library(haldensity)
library(haldensify)
help(haldensity)
help(haldensify)
rm(list=ls())
library(data.table)
setwd("~/UC Berkeley Biostat Dropbox/Alan Hubbard/hubbardlap/grantproposals/ki Gates/Synthetic data supp/GitSynth/pasc-sim/R")
library(data.table)
library(dplyr)
library(glmnet)
library(R6)
library(sl3)
library(tmle3)
library(hal9001)
rm(list = ls())
source("uhal_Alan_2.R")
source("SimSynFunctions_Alan.R")
set.seed(123)
df <- generate_data_simple(N=500)
node_list <- list(
W = setdiff(names(df), c("A", "Y")),
A = "A",
Y = "Y"
)
# Get True Values
psiP0 <- True_values()
## tmle3 and SL specs
ate_spec <- tmle_ATE(
treatment_level = 1,
control_level = 0
)
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_xgb <- make_learner(Lrnr_xgboost)
lrnr_earth <- make_learner(Lrnr_earth)
lrnr_glm <- make_learner(Lrnr_glm)
# define metalearners appropriate to data types
ls_metalearner <- make_learner(Lrnr_nnls)
lb_metalearner <- make_learner(Lrnr_solnp,
learner_function = metalearner_logistic_binomial,
loss_function = loss_loglik_binomial)
sl_Y <- Lrnr_sl$new(
learners = list(lrnr_mean, lrnr_glm, lrnr_earth),
metalearner = ls_metalearner
)
sl_A <- Lrnr_sl$new(
learners = list(lrnr_mean, lrnr_glm, lrnr_earth),
metalearner = lb_metalearner
)
learner_list <- list(A = sl_A, Y = sl_Y)
# HAL spec
### This name needs to be used
lrnr_uhal <- Lrnr_uhal9001$new()
lrnr_uhalcv <- Lrnr_uhal9001cv$new()
# Set up Sim
## Specs for SL and HAL and TMLE
# B is number of simulations
tmle.res <- NULL
simp.res <- NULL
df <- generate_data_simple(N=500)
df[1:10,]
# Load Libraries
library(hal9001)
library(glmnet)
library(stringi)
library(dplyr)
##simulate training data:
set.seed(1231)
library(ggplot2)
sim.true <- function() {
X <- seq(-4,4,length=500)
#stepwise function between X and Y
EY <- -1*(X > -3) + 3*(X > -2) + -2*(X > 0) + 4*(X>2) + -1*(X >3)
return(data.frame(X,EY)) }
dat <- sim.true()
plot(dat[,"X"],dat[,"EY"],xlab="x",ylab="E(Y|X)",type="s")
sim.dat <- function(n,sigma) {
X <- runif(n,-4,4)
#stepwise function between X and Y
Y <- -1*(X > -3) + 3*(X > -2) + -2*(X > 0) + 4*(X>2) + -1*(X >3)+
rnorm(n,0,sigma)
return(data.frame(X,Y)) }
dat <- sim.dat(100,0.50)
plot(dat$X,dat$Y,xlab="X",ylab="Y")
X <- as.matrix(dat$X)
Y <- as.vector(dat$Y)
hal.fit1 <- fit_hal(X,Y,family="gaussian",
X_unpenalized = NULL,
max_degree = ifelse(ncol(X) >= 20, 2, 3),
smoothness_orders = 0,
reduce_basis = 1 / sqrt(length(Y)),
fit_control = list(
cv_select = TRUE,
n_folds = 10,
foldid = NULL,
use_min = TRUE,
lambda.min.ratio = 1e-4,
prediction_bounds = "default"
),
basis_list = NULL,
return_lasso = TRUE,
return_x_basis = TRUE,
yolo = FALSE)
oo <- order(dat$X)
plotx <- dat$X[oo]
pred.fit1 <- predict(hal.fit1,new_data=plotx)
plot(dat$X,dat$Y)
lines(plotx,pred.fit1)
init_coef <-hal.fit1$coefs[-1]
nonzero_col <- which(init_coef != 0)
basis_mat <- as.matrix(hal.fit1$x_basis)
basis_mat <- as.matrix(basis_mat[, nonzero_col])
##  Get the cut-offs
cutoffs <- unlist(lapply(hal.fit1$basis_list, function (x) x['cutoffs']))
cutoffs <- cutoffs[nonzero_col]
coeff.not0 <- init_coef[nonzero_col]
intercpt <- hal.fit1$coefs[1]
##
form <- paste(round(coeff.not0,2),"*I(X >",round(cutoffs,2),")")
form <- stri_paste(form, collapse='+')
form <- paste(round(intercpt,2),form,sep="+")
form
generate_data_simple <- function(N,Wranges=c(-1,1,-1,1),betaA=c(0,0.1,-0.4),
betaY0=c(0,1,2,-1),betaC=c(1,7/5,5,3),sdy=1){
# A simple data generating process
W1 <- runif(N,Wranges[1],Wranges[2])
W2 <- runif(N,Wranges[3],Wranges[4])
pi0 <- plogis(betaA[1]+betaA[2]*W1*W2+betaA[3]*W1)
A <- rbinom(N,1,prob=pi0)
muY0 <- betaY0[1]+betaY0[2]*W1*W2 + betaY0[3]*W2^2 +betaY0[4]*W1
CATE <- betaC[1]*W1^2*(W1+betaC[2]) + (betaC[3]*W2/betaC[4])^2
muY = muY0+A*CATE
Y <- rnorm(N,sd=sdy,mean= muY)
return(tibble(W1=W1,W2=W2,A=A,Y=Y))
}
N=500
dat <- generate_data_simple(N)
True_value <- function(N=100000,Wranges=c(-1,1,-1,1), betaY0=c(0,1,2,-1),
betaC=c(1,7/5,5,3))
{
# A simple data generating process
W1 <- runif(N,Wranges[1],Wranges[2])
W2 <- runif(N,Wranges[3],Wranges[4])
muY0 <- betaY0[1]+betaY0[2]*W1*W2 + betaY0[3]*W2^2 +betaY0[4]*W1
CATE <- betaC[1]*W1^2*(W1+betaC[2]) + (betaC[3]*W2/betaC[4])^2
Y1 = muY0+CATE
muY1 = mean(Y1)
return(muY1)
}
True_value()
X <- as.matrix(dat[,-c(4)])
Y <- as.vector(dat$Y)
hal.fit2 <- fit_hal(X,Y,family="gaussian",
X_unpenalized = NULL,
max_degree = ifelse(ncol(X) >= 20, 2, 3),
smoothness_orders = 0,
reduce_basis = 1 / sqrt(length(Y)),
fit_control = list(
cv_select = TRUE,
n_folds = 10,
foldid = NULL,
use_min = TRUE,
lambda.min.ratio = 1e-4,
prediction_bounds = "default"
),
basis_list = NULL,
return_lasso = TRUE,
return_x_basis = TRUE,
yolo = FALSE)
pred.fit2 <- predict(hal.fit2,new_data=X)
plot(pred.fit2,Y, xlab="Predicted",ylab="Observed")
abline(0,1)
cor(pred.fit2,dat$Y)^2
dat.new <- generate_data_simple(N=500)
Xn <- as.matrix(dat.new[,-c(4)])
Yn <- as.vector(dat.new$Y)
pred.fitn <- predict(hal.fit2,new_data=Xn)
cor(pred.fitn,Yn)^2
init_coef <-hal.fit2$coefs
nonzero_col <- which(init_coef != 0)
length(init_coef[nonzero_col])
hal.fit2b <- fit_hal(X,Y,family="gaussian",
X_unpenalized = NULL,
max_degree = ifelse(ncol(X) >= 20, 2, 3),
smoothness_orders = 1,
reduce_basis = 1 / sqrt(length(Y)),
fit_control = list(
cv_select = TRUE,
n_folds = 10,
foldid = NULL,
use_min = TRUE,
lambda.min.ratio = 1e-4,
prediction_bounds = "default"
),
basis_list = NULL,
return_lasso = TRUE,
return_x_basis = TRUE,
yolo = FALSE)
length(hal.fit2$coefs)
init_coef <-hal.fit2b$coefs
nonzero_col <- which(init_coef != 0)
length(init_coef[nonzero_col])
Xn[,3] <- 1
pred.fitEY1 <- predict(hal.fit2b,new_data=Xn)
phihat <-mean(pred.fitEY1)
X[1:10,]
Xn[1:10,]
Xn <- as.matrix(dat.new[,-c(4)])
Xn[1:3,]
X[1:3,]
N=500
dat <- generate_data_simple(N)
dat[1:10,]
dat[1:15,]
dat[1:5,]
X <- as.matrix(dat[,-c(4)])
Y <- as.vector(dat$Y)
hal.fit2b <- fit_hal(X,Y,family="gaussian",
X_unpenalized = NULL,
max_degree = ifelse(ncol(X) >= 20, 2, 3),
smoothness_orders = 1,
reduce_basis = 1 / sqrt(length(Y)),
fit_control = list(
cv_select = TRUE,
n_folds = 10,
foldid = NULL,
use_min = TRUE,
lambda.min.ratio = 1e-4,
prediction_bounds = "default"
),
basis_list = NULL,
return_lasso = TRUE,
return_x_basis = TRUE,
yolo = FALSE)
Xn <- X
Xn[,3] <- 1
pred.fitEY1 <- predict(hal.fit2b,new_data=Xn)
phihat <-mean(pred.fitEY1)
phihat
dim(IC_beta)
coef <- hal.fit2b$coefs
# Need to add intercept to match dimension of coef
basis_mat <- cbind(1, as.matrix(hal.fit2b$x_basis))
nonzero_idx <- which(coef != 0)
coef_nonzero <- coef[nonzero_idx]
basis_mat_nonzero <- as.matrix(basis_mat[, nonzero_idx])
# Get predicted Y (from above)
Y_hat <- predict(hal.fit2b,new_data=X)
# IC_beta is a pxn matrix, where each row is the IC for a particular beta.
IC_beta <- cal_IC_for_beta(X = basis_mat_nonzero,
Y = Y,
Y_hat =Y_hat,
beta_n = coef_nonzero)
cal_IC_for_beta <- function(X, Y, Y_hat, beta_n){
n <- dim(X)[1]
p <- length(beta_n)
if (!is.matrix(X)) X <- as.matrix(X)
# 1. calculate score: X'(Y - phi(X))
res <- Y-Y_hat
score <- sweep(t(X), 2, res, `*`)
# 2. calculate the derivative of phi:
d_phi = - X
# 3. -E_{P_n}(X d_phi)^(-1)
tmat <- t(X) %*% d_phi / n
if(! is.matrix(try(solve(tmat), silent = TRUE))){
return(NA)
}
tmat <- -solve(tmat)
# 4. calculate influence curves
IC <- tmat %*% score
return(IC)
}
IC_beta <- cal_IC_for_beta(X = basis_mat_nonzero,
Y = Y,
Y_hat =Y_hat,
beta_n = coef_nonzero)
dim(IC_beta)
Xn[1:3,]
X[1:3,]
X_new <- make_design_matrix(Xn, hal.fit2b$basis_list, p_reserve = 0.75)
X_new <- cbind(1, X_new)
X_new <- as.matrix(X_new[, nonzero_idx])
dim(X_new)
dim(basis_mat)
dim(basis_mat_nonzero
)
cal_IC_for_EY <- function(X_new, IC_beta){
if (!is.matrix(X_new)) X_new <- as.matrix(X_new)
d_phi_new = X_new
IC = colMeans(d_phi_new %*% IC_beta)
return(IC)
}
ICEY1 <- cal_IC_for_EY(X_new,IC_beta)
length(ICEY1)
n <- length(ICEY1)
SE <- sqrt(mean(ICEY1^2)/n)
phihat+1.96*SE
phihat-1.96*SE
