---
title: "Synthetic Data Simulation Results"
author: "Alan Hubbard"
date: "`r Sys.Date()`"
output: pdf_document
#output: slidy_presentation
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=F, message=F,warning=F}
library(data.table)
library(dplyr)
library(glmnet)
library(sl3)
library(tmle3)
library(R6)
library(hal9001)
library(cowplot)
library(ggplot2)

rm(list = ls())
source("uhal_Alan.R")
source("SimSynFunctions_Alan.R")
```

## Simulation Code

```{r, eval=F, echo=T, message=F,warning=F}
generate_data_simple <- function(N,Xranges=c(-1,1,-1,1),betaA=c(0,0.1,-0.4),
                                 betaY0=c(0,1,2,-1),betaC=c(1,7/5,5,3),sdy=1){
  # A simple data generating process
  X1 <- runif(N,Xranges[1],Xranges[2])
  X2 <- runif(N,Xranges[3],Xranges[4])
  pi0 <- plogis(betaA[1]+betaA[2]*X1*X2+betaA[3]*X1)
  A <- rbinom(N,1,prob=pi0)
  muY0 <- betaY0[1]+betaY0[2]*X1*X2 + betaY0[3]*X2^2 +betaY0[4]*X1
  CATE <- betaC[1]*X1^2*(X1+betaC[2]) + (betaC[3]*X2/betaC[4])^2
  muY = muY0+A*CATE
  Y <- rnorm(N,sd=sdy,mean= muY)
  return(tibble(X1=X1,X2=X2,A=A,Y=Y))
}
```


## Synthetic data simulation details

We have created a structure to 

_1. Simulate complex data from known distributions
  
    _a. Can be cross-sectional,
  
    _b. longitudinal, 
  
    _c. have missing data, etc.
  
_2. Define a set of parameters we wish to estimate and derive inference from the synthetic data

    _a. Marginal parameters, like means
    
    _b. Regression estimates for working models
    
    _c. Causal parameters based on known causal model

## Simulation Structure

Investigate the performance of competing methods for synthetic data by:
 
_1. Simulate the data.
 
_2. Estimate the data-generating distribution using different methods (HAL, SuperLearner, etc.): looking at additional methods.

_3. Synthethesize the data from the estimated DGDs based on  competing methods.

_4. Repeat 1-3 1000 times.

_5. Evaluate the comparison of the distribution of synthetic data-based results to those based on the actual data.
  
## X-Sectional Data Example   

- Data: $O=(W,A,Y)$

- Causal Model: $W \rightarrow A \rightarrow Y$

- $W_1, W_2 \sim Uniform$

- Complex DGD

    - $logit(P(A=1|W)) = \alpha_0+\alpha_1* A+\alpha_2*A*W_1*W_2+\alpha_3*W_1$

    - $E(Y_0|W) = \beta_0+\beta_1*W_1*W_2+\beta_2*W_2^2+\beta_3*W_1$

    - $E(Y_1|W)-E(Y_0|W)= \gamma_0+\gamma_1*W_1^2*(W_1+\gamma_2) + \gamma_3*W_2^2$
  
    - $Y = E(Y_0|W)+A*(E(Y_1|W)-E(Y_0|W))+e, e \sim N(0,\sigma)$

## Parameters of Interest

- Simple ones
  
    - $P(A=1)$

    - $EY$
    
- Standard Regression parameters

    - Coefficients in working model: 
    $b_0+b_1*W_1+b_2*W_2+b_3*A$

- Causal parmaters estimated with (targeted) machine learning

  - $ATE=E(E(Y|A=1,W)-E(Y|A=0,W))$
  
## Methods for generating Synthetic Data

- Undersmoothed Highly adaptive lasso (HAL) - undersmoothing based on ATE

- SuperLearner

- Compare estimates using synthetic data to those based on the data behind synthetic data

## Estimators used to get parameters from synthetic (and actual) data
- Simple parameters -  simple averages

- Coefficients - ordinary least squares

- ATE - TMLE with SL

- Evaluations: bias, variance, MSE and coverage

## Straightforward issues with inference from Synthetic Data
- If the methods used to estimate the parameters that define the model for the DGD are unbiased, synthetic data based on them will not be.

- For example, if my model is $X \sim N(\mu,\sigma)$
    
- Want to synthesize data based on a sample  $X_i, i=1,...,n$
    
- Generate synthetic data from  $N(\bar{X},\sigma): X^*_i, i=1,...n$

- Estimate $\mu$ with $\bar{X}^*$

- MSE of estimate based on synthetic data is: $MSE (\bar{X}^*)=E(\bar{X}^*-\bar{X})^2+E(\bar{X}-\mu)^2$

- To get good coverage in this simple conrext, would increase the margin of error in confidence intervals by a factor of $\sqrt{2}: \hat{\theta} \pm \sqrt{2}*1.96*SE(\hat{\theta})$.

- Could also generate a synthetic data sample that is very large relative to the size of the original data to get estimates, but based inference (calculate standard erros) on a synthetic sample the same size as the original data.




  
```{r, echo=F, message=F,warning=F}
## Load simulation results
load("~/UC Berkeley Biostat Dropbox/Alan Hubbard/hubbardlap/grantproposals/ki Gates/Synthetic data supp/GitSynth/pasc-sim/R/April19.rdata")

reg.nmes <- c("int","A","X1","X2")
nmes <- c("ave.dat.A","ave.dat.Y","ave.hal.A",
            "ave.hal.Y","ave.halcv.A",
            "ave.halcv.Y","ave.sl.A","ave.sl.Y",
            paste0("regest.dat.",reg.nmes),
            paste0("regSE.dat.",reg.nmes),
            paste0("regest.hal.",reg.nmes),
            paste0("regSE.hal.",reg.nmes),
            paste0("regest.halcv.",reg.nmes),
            paste0("regSE.halcv.",reg.nmes),
            paste0("regest.sl.",reg.nmes),
            paste0("regSE.sl.",reg.nmes))

colnames(simp.res) <- nmes

tnmes <- c("est.","se.")            
colnames(tmle.res) <- c(paste0(tnmes,"dat"),
                        paste0(tnmes,"hal"),
                        paste0(tnmes,"halcv"),
                        paste0(tnmes,"sl"))
# True values psiP0<-list(P0A=P0A,P0Y=P0Y,true.ATE,coef.work)

```



```{r, echo=F, message=F,warning=F}
simp.res <- data.frame(simp.res)
marginsA <- simp.res[,c(1,3,5,7)]
marginsY <- simp.res[,c(2,4,6,8)]

kp <- c("regest.dat.A","regSE.dat.A","regest.hal.A","regSE.hal.A",
        "regest.halcv.A","regSE.halcv.A","regest.sl.A","regSE.sl.A") 

regrA <- simp.res[,kp]

in.CI <- function(x,tru) {
  xL <- x[1]-1.96*x[2]
  xH <- x[1]+1.96*x[2]
  return(tru > xL & tru < xH)
}

bias.var.MSE <- function(x,tru) {
  bias <- mean(x)-tru
  varx <- var(x)
  MSE <- bias^2+varx
  return(c(bias,varx,MSE))
}

res.bias <- apply(marginsA,2,bias.var.MSE,tru=psiP0$P0A)
rownames(res.bias) <- c("bias","var","MSE")
# Re-arrange for plotting
bias.plt <- res.bias[-c(3),]
bias.plt[1,] <- (bias.plt[1,])^2
estim <- c(rep("data",2),rep("hal.und",2),rep("hal.cv",2),rep("SL",2))
err <- rep(c("bias2","var"),4)
titl <- paste0("MSE and Squared Bias for 3 Estimators of P(A=1) = ",
               round(psiP0$P0A,2))
plt.bias <- data.frame(Amount=as.vector(bias.plt),estim=estim,err=err)
plt.A <- ggplot(plt.bias, aes(x = estim, y = Amount, fill = err)) +  
  geom_bar(stat = "identity")+ggtitle(titl)


res.biasY <- apply(marginsY,2,bias.var.MSE,tru=psiP0$P0Y)
rownames(res.biasY) <- c("bias","var","MSE")
# Re-arrange for plotting
bias.plt <- res.biasY[-c(3),]
bias.plt[1,] <- (bias.plt[1,])^2
estim <- c(rep("data",2),rep("hal.und",2),rep("hal.cv",2),rep("SL",2))
err <- rep(c("bias2","var"),4)
titl <- paste0("MSE and Squared Bias for 3 Estimators of E(Y) = ",
               round(psiP0$P0Y,2))
plt.bias <- data.frame(Amount=as.vector(bias.plt),estim=estim,err=err)
plt.Y <- ggplot(plt.bias, aes(x = estim, y = Amount, fill = err)) +  
  geom_bar(stat = "identity")+ggtitle(titl)

# Regression coefficient on misspecified model for E(Y|A,X1,X2)
regrA.est <- regrA[,c(1,3,5,7)]
res.bias.coeff <- apply(regrA.est,2,bias.var.MSE,tru=psiP0[[4]][2])
rownames(res.bias.coeff) <- c("bias","var","MSE")

bias.plt <- res.bias.coeff[-c(3),]
bias.plt[1,] <- (bias.plt[1,])^2
titl <- paste0("MSE and Squared Bias for 3 Estimators of A coeff = ",
               round(psiP0[[4]][2],2))
plt.bias <- data.frame(Amount=as.vector(bias.plt),estim=estim,err=err)
plt.regA <- ggplot(plt.bias, aes(x = estim, y = Amount, fill = err)) +  
  geom_bar(stat = "identity")+ggtitle(titl)

# Coverage
covr <- mean(apply(regrA[,1:2],1,in.CI,tru=psiP0[[4]][2]))
covr <- c(covr, mean(apply(regrA[,3:4],1,in.CI,tru=psiP0[[4]][2])))
covr <- c(covr, mean(apply(regrA[,5:6],1,in.CI,tru=psiP0[[4]][2])))

covr <- c(covr, mean(apply(regrA[,7:8],1,in.CI,tru=psiP0[[4]][2])))


dat.cvr <- data.frame(estim = c("data","hal.und","hal.cv","SL"),coverage=covr)

cutoff <- data.frame(yintercept=0.95, Lines='95 percent coverage')

plt.cvr <- ggplot(dat.cvr, aes(x = estim, y = coverage)) +  
  geom_bar(stat = "identity")+ggtitle("Coverage of 95% CI for coefficient on A")+ylim(0,1)+
  geom_hline(aes(yintercept=yintercept, linetype=Lines), cutoff,linetype='dashed', color='red')

### tmle results
tmle.est <- tmle.res[,c(1,3,5,7)]
res.bias.coeff <- apply(tmle.est,2,bias.var.MSE,tru=psiP0[[3]])
rownames(res.bias.coeff) <- c("bias","var","MSE")

bias.plt <- res.bias.coeff[-c(3),]
bias.plt[1,] <- (bias.plt[1,])^2
titl <- paste0("MSE and Squared Bias for 3 Estimators of ATE = ",
               round(psiP0[[3]],2))
plt.bias <- data.frame(Amount=as.vector(bias.plt),estim=estim,err=err)
plt.tmle <- ggplot(plt.bias, aes(x = estim, y = Amount, fill = err)) +  
  geom_bar(stat = "identity")+ggtitle(titl)

# Coverage
covr <- mean(apply(tmle.res[,1:2],1,in.CI,tru=psiP0[[3]]))
covr <- c(covr, mean(apply(tmle.res[,3:4],1,in.CI,tru=psiP0[[3]])))
covr <- c(covr, mean(apply(tmle.res[,5:6],1,in.CI,tru=psiP0[[3]])))

covr <- c(covr, mean(apply(tmle.res[,7:8],1,in.CI,tru=psiP0[[3]])))


dat.cvr <- data.frame(estim = c("data","hal.und","hal.cv","SL"),coverage=covr)

cutoff <- data.frame(yintercept=0.95, Lines='95 percent coverage')

plt.cvr.tmle <- ggplot(dat.cvr, aes(x = estim, y = coverage)) +  
  geom_bar(stat = "identity")+ggtitle("Coverage of 95% CI for ATE")+ylim(0,1)+
  geom_hline(aes(yintercept=yintercept, linetype=Lines), cutoff,linetype='dashed', color='red')

```


# Results

## $P(A=1)$

```{r, echo=F, message=F,warning=F}
plt.A
```

## $EY$
```{r, echo=F, message=F,warning=F}
plt.Y
```

## Regression coefficients from working model

```{r, echo=F, message=F,warning=F}
plt.regA
plt.cvr
```

## ATE

```{r, echo=F, message=F,warning=F}
plt.tmle
plt.cvr.tmle
```

## Other work in progress

- More complex simulations that mimic N3C data (e.g., missing data, more complex models based on long Covid N3C data)

- Methods for obscuring  baseline covariates for privacy

    - Estimating joint distribution of discretized baseline covariates
    
    - Apply required coarsening of identifying variables (age, etc.)
    
- Comparisons to existing synthetic data algorithms available (usually very similar in framework, but use more arbitrary modeling assumptions)

- Comparisons to CV-HAL

- Develop general algorithm that works directly on HAL objects and users can querry parameters by providing functions which operate on HAL objects that contain DGD components.

- Exploring generative AI but skeptical they are applicable to sample sizes of data based on N3C without having a pre-trained model on similar data.
